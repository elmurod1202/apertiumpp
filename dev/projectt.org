
# -*- mode: org; -*-

#+TITLE:      PROJECT T, or APERTIUM++!
#+AUTHOR:      
#+DATE:        
#+EMAIL:       
#+LANGUAGE:    en
#+OPTIONS:     H:5 num:5 toc:t \n:nil ::t |:t ^:t f:t tex:t ...
#+BIBLIOGRAPHY: refs plain

This page documents research carried over in the [[https://taruen.github.io/apertiumpp/][Apertiumpp project]].

* OBJECTIVES

Enabling machine translation:

1. from any Turkic language to any other Turkic language,
2. between any Turkic language and English, and
3. between any Turkic language and Russian.

What we're after:

1. Machine translators themselves.
2. Description of the process and infrastructure for solving the same problem
   for another family of languages in the most efficient manner.
3. Better yet, a program which asks native speakers the right questions in the
   right order, judges whether answers are reliable, and writes machine
   translators for us.
4. Text-to-speech systems.
5. Speech recognition systems.
6. Annotated corpora with a standard interface.
7. Spellcheckers (and keyboard layouts, if missing).

Somewhat more formally:

#+begin_problem
(*Begin Problem 1*

Given:

Set $L = \{l_1, l_2, ..., l_n\}$ of languages of one language family.

Unknown:

Algorithm for obtaining machine translators $t_i ... t_{n * (n-1)+2n}$ from
$l_1$ to $l_2$ where $l_1 \in L$ and $l_2 \in L \cup \{eng, rus\}$ and machine
translators themselvelves, including spech-to-text and text-to-speech frontends.

Conditions:

1) $Word Error Rate(t_i) < THRESHOLD$ for every $i$.
2) As a byproduct, the process generates a standalone morphological analyzer
   (applicable as a spellchecker), dependency and/or semantic parser, speech
   recognizer and speech synthesizer for every $l \in L$.
*End Problem 1*)
#+end_problem

In our case, $L = \{kaz, tat, kir, tyv, tur, chv, kum, kaa, uzb, sah, crh, krc,
 bak, nog, gag, tuk, uig, alt, kjh, ota, aze\}$.

We want a connected graph (not necessarily complete, if we pivot) consisting of
the nodes in $L$ + a path from every node in $L$ to $eng$ and $rus$. Two nodes
are considered connected if there is a ([[http://wiki.apertium.org/wiki/Mixed_modes][mixed]]) [[http://wiki.apertium.org/wiki/Modes][mode]] for translating between them
(in both directions).

* RATIONALE

Applications:
- translating Khanacademy's content (Creative Commons BY-NC-SA 3.0 US)
- translating (English) Wikipedia (Creative Commons BY-SA 3.0 Unported)
  - [[https://meta.wikimedia.org/wiki/List_of_articles_every_Wikipedia_should_have][articles every Wikipedia should have]]
  - selected articles of other Wikipedias
- localizing free/libre software
- translating other resources listed on [[http://selimcan.org]]
- components used as spell and grammar checkers and in QA systems

* WHAT WAS DONE

See [[http://wiki.apertium.org/wiki/Turkic_languages]].

* WORKING ON / NEEDED

** Resources

- multilingual dictionary
  - closed class words
  - nouns, adjectives, verbs, adverbs
  - proper nouns / named entities
- parallel corpus
- parallel tagged corpus
- parallel treebank
- multilingual dictionary of idioms (a translation memory)
- spoken texts (under CC0, CC-BY, CC-BY-SA or compatible licenses)
- a mobile app which collects all of the above from the volunteer contributors

*** Multilingual dictionary

**** RATIONALE

$n$ languages make up $\sum_{i=1}^{n-1} i$ bilingual machine translators. For 21
languages in $L$ listed above, $\sum_{i=1}^{21-1} i = 210$. We don't want to
maintain $\sum_{i=1}^{n-1} i$ separate bidixes for translating between $n$
languages (even not considering $rus$ and $eng$, no pivoting, 210 for
$n=21$). In fact, we probably don't wan't to maintain 20 dictionaries with the
same left side either (always pivoting through the same language), but that
might be feasible.

**** OBJECTIVES

A multilingual dictionary from which individual monolingual and
bilingual dictionaries can be generated.

TODO a representation/format for the multidix
- see [[http://wiki.apertium.org/wiki/User:Unhammer/wishlist][Unhammer's wishlist]] for details
- entries contain:
  - stem
  - the LEXICON(s)/pardef(s) the stem should be linked to in the monodix
  - tags which should end up in the bidix (or a bidix pardef)
  - alternative or erroneous spellings (LR/RL forms)
  - other attributes (language variant, style, archaic or not etc)
  - example sentences with the word in question
- options:
  - a DTD based on dix.dtd?

TODO a program which converts Apertium mono- and bidixes into a multidix

See [[wordgraph.py]] and [[enwordnet2twordnet.py]].

TODO a program which converts a multidix into Apertium monodixes and bidixes

**** CONSTRAINTS

- should be writable by monolingual Turkic speakers (speakers of only
  one Turkic language, that is. Knowledge of English or Russian is assumed,
  since otherwise the only reliable linkage would be pictures).

**** OPTIONS

- pivoting
  - translating entries from a public domain/libre English dictionary
    - Wordnet? GNU collaborative dictionary of English? Wiktionary?
      OmegaWiki?
  - same for Russian (if there is any available...)
    - TODO check [https://en.wikipedia.org/wiki/Ushakov_Dictionary] might be in
      the public domain now
  - translating entries from a libre Turkic dictionary
    - is there any? Probably not.
      - DONE email publishers ([[http://www.twirpx.com/file/1077154][this]] one is of interest).
        - Ibragimov Institute of Language, Literature and Art @ Tatarstan
          Academy of Sciences in email communication allowed entry words and
          part-of-speech tags from the currently 3-volume (А-К) ``Татар теленең
          аңлатмалы сүзлеге'' (``Explanatory Dictionary of Tatar'') found here
          http://www.antat.ru/tt/iyli/publishing/book/index.php to be used in
          Apertium's dictionaries.
    - even if there is one, allowing a monolingual Turkic speaker to translate
      words into his own language will require translating the *definitions* into
      English or Russian, which is a huge amount of work compared to translating
      the words only

- inducing from bilingual corpora

- inducing from monolingual corpora
  - Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008,
    June). Learning Bilingual Lexicons from Monolingual Corpora. In
    ACL (Vol. 2008, pp. 771-779).
  - Koehn, P., & Knight, K. (2002, July). Learning a translation
    lexicon from monolingual corpora. In Proceedings of the ACL-02
    workshop on Unsupervised lexical acquisition-Volume 9
    (pp. 9-16). Association for Computational Linguistics.
  - ...

- currently apertium-eng-kaz.eng-kaz.dix has roughly
  #+name: eng-kaz-entries
  #+begin_src sh :exports both
  grep -c "<e>" ../apertium-all/apertium-trunk/apertium-eng-kaz/apertium-eng-kaz.eng-kaz.dix
  #+end_src

  #+RESULTS: eng-kaz-entries
  : 32886

  entries in it.

- also see: [[http://wiki.apertium.org/wiki/Bilingual_dictionary_discovery]]

***** Wordnet

- experiences with translating English Wordnet into another language?
  - Lindén, K., & Carlson, L. (2010). FinnWordNet–Finnish WordNet by
    Translation. LexicoNordica–Nordic Journal of Lexicography, 17,
    119-140.
  - Lindén, K., & Niemi, J. (2014). Is it possible to create a very large
    wordnet in 100 days? An evaluation. Language resources and evaluation,
    48(2), 191-201.
  - Isahara, H., Bond, F., Uchimoto, K., Utiyama, M., & Kanzaki,
    K. (2008). Development of the Japanese WordNet.
  - Niemi, J., Lindén, K., & Hyvärinen, M. (2012, January). Using a Bilingual
    Resource to Add Synonyms to a Wordnet. In Proceedings of the Global Wordnet
    Conference.
  - Bond, F., Isahara, H., Kanzaki, K., & Uchimoto, K. (2008). Boot-strapping a
    WordNet using multiple existing WordNets.
  - [[http://compling.hss.ntu.edu.sg/omw/][Open Multilingual Wordnet]]
  - [[http://globalwordnet.org][Global WordNet Association]]
- pros: free license, no need to scan anything, good for papers

**** WHAT WAS DONE

***** A library for converting two or more Apertium bidixes into a wordgraph

#+name: wordgraph.py
#+begin_src python :exports yes :results output :tangle wordgraph.py

"""
wordgraph.py

A library for converting two or more Apertium bidixes into a Wordgraph (its
definition you can see below) and then doing various things with that
wordgraph such as:
- exporting it as a Multidix, in which entries are *optionally* linked to
  English Wordnet's definitions (see bidixes2multidix.py),
- translating English Wordnet lemmas to other languages via (chain) lookup
  in the wordgraph or in Google/Yandex translate (see enwordnet2twordnet.py),
- or generating new bidixes for language pairs for which you didn't have a
  bidix before (TODO).

USAGE: import wordgraph as wg

TODO:
  - handle LR RL restrictions
"""

import xml.etree.ElementTree as ET
from xml.dom import minidom
import glob
import os.path
from collections import namedtuple, defaultdict
from io import StringIO
import re
from copy import deepcopy
import sys


## Constants
## =========


ISO2_2_ISO3 = {'kz': 'kaz', 'tt': 'tat', 'ky': 'kir', 'tr': 'tur', 'cv': 'chv',
               'uz': 'uzb', 'ba': 'bak', 'tk': 'tuk', 'ug': 'uig', 'az': 'aze',
               'en': 'eng'}
ISO3_2_ISO2 = {'kaz': 'kk', 'tat': 'tt', 'kir': 'ky', 'tur': 'tr', 'chv': 'cv',
               'uzb': 'uz', 'bak': 'ba', 'tuk': 'tk', 'uig': 'ug', 'aze': 'az',
               'eng': 'en'}


## Data definitions
## ================


MonolingEntry = namedtuple("MonolingEntry", ["lang", "lm", "tags"])
##
## MonolingEntry is MonolingEntry(String, String, (Tuple of String))
## interp.: a monolingual dictionary entry, where:
##          - lang is iso3 code of the language
##          - lm is the lemma
##          - tags are the symbols used in Apertium to denote part-of-speech
##            tags and other morphological features (the ones which you'd
##            put into a bidix)

MONOLING_E_1 = MonolingEntry("eng", "", ())  # null translation
MONOLING_E_2 = MonolingEntry("eng", "file", ("n",))
MONOLING_E_3 = MonolingEntry("kaz", "файл", ("n",))
MONOLING_E_4 = MonolingEntry("kaz", "егеу", ("n",))
MONOLING_E_5 = MonolingEntry("tat", "игәү", ("n",))
MONOLING_E_6 = MonolingEntry("eng", "Moscow", ("np", "top"))
MONOLING_E_7 = MonolingEntry("tat", "Мәскәү", ("np", "top", "hargle"))
MONOLING_E_8 = MonolingEntry("rus", "Москва", ("np",))
MONOLING_E_9 = MonolingEntry("tur", "Moskova", ())


## A Graph is a Dictionary which maps Object to a (Set of Object).
## interp.: {node: {its, neighbouring, nodes}

G_1 = {'a': {'b', 'c'},                     ## a---b---d---f
       'b': {'a', 'c', 'd'},                ##  \ /
       'c': {'a', 'b'},                     ##   c     g  h---i
       'd': {'b', 'f'},
       'f': {'d'},
       'g': {},
       'h': {'i'},
       'i': {'h'}}


## WordGraph is a Graph which maps MonolingEntry to
## a (Set of MonolingEntry)
## interp.: {monoling_e_1: {monoling_e_2, monoling_e_3},
##           monoling_e_2: {monoling_e_1},
##           monoling_e_3: {monoling_e_1}}
##
##   means that (monoling_e_1 and monoling_e_2), and
##   (monoling_e_1 and monoling_e_3) were translations of each other in a bidix.

WG_1 = {MONOLING_E_2: {MONOLING_E_3, MONOLING_E_4},
        MONOLING_E_3: {MONOLING_E_2},
        MONOLING_E_4: {MONOLING_E_2}}

WG_2 = {MONOLING_E_2: {MONOLING_E_3, MONOLING_E_4},
        MONOLING_E_3: {MONOLING_E_2},
        MONOLING_E_4: {MONOLING_E_2, MONOLING_E_5},
        MONOLING_E_5: {MONOLING_E_4}}

WG_3 = {MONOLING_E_6: {MONOLING_E_7, MONOLING_E_8, MONOLING_E_9},
        MONOLING_E_7: {MONOLING_E_6},
        MONOLING_E_8: {MONOLING_E_6},
        MONOLING_E_9: {MONOLING_E_6}}


## Functions
## =========

def main(main_bidix, iso_codes):
    """ String (List of String) -> String

    Given the path to the main bidix (read: biggest English-to-X or
    X-to-English dictionary) and a list of iso3 codes of relevant languages,
    construct a multidix, in which English words are linked to
    their Wordnet definitions (in case of nouns, adjectives, verbs and
    adverbs) and their translations to languages listed in iso_codes, and
    return a string representation of that multidix (read: xml).

    A word is considered a translation of the English word if there exists
    a path between the two in the WordGraph constructed out of the bidixes.
    """
    wg = bidixes2wordgraph(
        append_leftiso3_rightiso3(
            get_bidixes(iso_codes)))

    bidix = ET.parse(main_bidix)
    root = bidix.getroot()
    for e in root.iter('e'):
        try:
            left, right = pair2monolings(e[0], 'eng', 'kaz')
        except IndexError:  # <e><re>...</re><p>...</p>
            left, right = pair2monolings(e[1], 'eng', 'kaz')
        if left.lm and len(left.tags) >= 1:
            if left.tags[0] in {'n', 'v', 'adj', 'adv'}:
                for defn in \
                  [synset.definition() for synset in \
                    wn.synsets(left.lm,
                               APERTIUMPOS_2_WNPOS[left.tags[0]])]:
                    if e.text:
                        e.text +=(defn + '\n')
                    else:
                        e.text = defn + '\n'
        e.append(deepcopy(monolinge_2_iso3element(left)))
        e.append(deepcopy(monolinge_2_iso3element(right)))
        for monoling_e in wg_connections(wg, left):
            e.append(deepcopy(monolinge_2_iso3element(monoling_e)))
        for p in e.iter('p'):
            e.remove(p)
 
    return minidom.parseString(ET.tostring(root)).toprettyxml(indent="  ",
                                                              newl="\n")


def manytags2singletag(wg):
    """ WordGraph -> WordGraph

    Iterate through all nodes (= MonolingEntries) of wg and, if
    a monolingentry.tags has many tags, limit it to a single tag
    (part-of-speech tag).
    """
    def _manytags2singletag(me):
        if len(me.tags) > 1:
            return MonolingEntry(me.lang, me.lm, me.tags[:1])
        else:
            return me

    res = defaultdict(set)
    for me in wg:
        if len(me.tags) > 1:
            for neibr in wg[me]:
                res[_manytags2singletag(me)].add(_manytags2singletag(neibr))
        else:
            for neibr in wg[me]:
                res[me].add(_manytags2singletag(neibr))
    return res
 
def test_manytags2singletag():
    assert manytags2singletag(WG_3) == \
        {MonolingEntry("eng", "Moscow", ("np",)):
            {MonolingEntry("tat", "Мәскәү", ("np",)),
             MonolingEntry("rus", "Москва", ("np",)),
             MonolingEntry("tur", "Moskova", ())},
         MonolingEntry("tat", "Мәскәү", ("np",)):
             {MonolingEntry("eng", "Moscow", ("np",))},
         MonolingEntry("rus", "Москва", ("np",)):
             {MonolingEntry("eng", "Moscow", ("np",))},
         MonolingEntry("tur", "Moskova", ()):
             {MonolingEntry("eng", "Moscow", ("np",))}}


def g_connections(graph, start_node):
    """ Graph -> (Generator Object)

    Traverse the graph (avoiding cycles) starting with start_node and yield
    all nodes the start node is connected to.
    """
    frontier = set()
    seen = {start_node}
    for neighbour in graph[start_node]:
        frontier.add(neighbour)
    while frontier:
        current = frontier.pop()
        if current not in seen:
            yield current
            seen.add(current)
            for neighbour in graph[current]:
                frontier.add(neighbour)
        else:
            continue

def test_g_connections():
    assert list(g_connections(G_1, 'g')) == []
    assert list(g_connections(G_1, 'h')) == ['i']
    assert sorted(g_connections(G_1, 'i')) == ['h']
    assert sorted(g_connections(G_1, 'a')) == ['b', 'c', 'd', 'f']
    assert sorted(g_connections(G_1, 'c')) == ['a', 'b', 'd', 'f']


def wg_connections(graph, start_node):
    """ WordGraph -> (Generator MonolingEntry)

    Traverse the graph (avoiding cycles) starting with start_node and yield
    all nodes the start node is connected to.
    """
    frontier = set()
    seen = {start_node.lang}
    for neighbour in graph[start_node]:
        frontier.add(neighbour)
    while frontier:
        current = frontier.pop()
        if current.lang not in seen:
            yield current
            seen.add(current.lang)
            for neighbour in graph[current]:
                if neighbour.lang not in seen:
                    frontier.add(neighbour)
        else:
            continue

def test_wg_connections():
    assert sorted(g_connections(WG_2, MONOLING_E_2)) ==\
           sorted([MONOLING_E_3,
                   MONOLING_E_4,
                   MONOLING_E_5])


def bidixes2wordgraph(bidixes):
    """ (List of (String, String, String) -> WordGraph

    Given a list of (bidix file name, lang1 iso3 code, lang 2 iso3 code)
    tuples, return a WordGraph with all stems contained in those bidix files.
    """
    res = defaultdict(set)
    for bidix, left_lang, right_lang in bidixes:
        try:
            bidix_root = ET.parse(bidix).getroot()
        except ET.ParseError:
            print("Couldn't parse ", bidix, ". Ill-formed xml?",
                  file=sys.stderr)
            continue
        for pair in bidix_root.iter('p'):
            left, right = pair2monolings(pair, left_lang, right_lang)
            res[left].add(right)
            res[right].add(left)
    return res

def test_bidixes2wordgraph():
    eng_kaz = StringIO(u"""<?xml version="1.0" encoding="UTF-8"?>
                     <dictionary>
                       <alphabet></alphabet>
                       <sdefs>
                         <sdef n="n"               c="Noun"/>
                       </sdefs>

                       <section id="main" type="standard">
                         <e><p><l>file<s n="n"/></l><r>файл<s n="n"/></r></p></e>
                         <e><p><l>file<s n="n"/></l><r>егеу<s n="n"/></r></p></e>
                       </section>
                     </dictionary>""")
    kaz_tat = StringIO(u"""<?xml version="1.0" encoding="UTF-8"?>
                     <dictionary>
                       <alphabet></alphabet>
                       <sdefs>
                         <sdef n="n"               c="Noun"/>
                       </sdefs>

                       <section id="main" type="standard">
                         <e><p><l>егеу<s n="n"/></l><r>игәү<s n="n"/></r></p></e>
                       </section>
                     </dictionary>""")

    assert bidixes2wordgraph([(eng_kaz, "eng", "kaz"),
                              (kaz_tat, "kaz", "tat")]) == WG_2


def pair2monolings(pair, left_lang, right_lang):
    """ ElementTree.Element String String -> (MonolingEntry, MonolingEntry)

    Extract the <l>eft and <r>ight hand sides from a <p>air element.
    """
    return MonolingEntry(left_lang,
                         ' '.join(pair[0].itertext()),
                         tuple(s.attrib['n'] for s in pair[0].iter('s'))), \
           MonolingEntry(right_lang,
                         ' '.join(pair[1].itertext()),
                         tuple(s.attrib['n'] for s in pair[1].iter('s')))

def test_pair2monolings():
    assert pair2monolings(ET.fromstring("""<p><l>file<s n="n"/></l><r>файл<s n="n"/></r></p>"""), "eng", "kaz") == \
           (MONOLING_E_2, MONOLING_E_3)


def monolinge_2_iso3element(monoling_e):
    """ MonolingEntry -> ElementTree.Element

    Convert the given monolingual entry into a xml element to be put
    inside of <e> in the final multidix.
    """
    res = ET.Element(monoling_e.lang)
    res.text = monoling_e.lm
    for tag in monoling_e.tags:
        ET.SubElement(res, 's', {'n': tag})
    return res

def test_monolinge_2_iso3element():
    assert ET.tostring(monolinge_2_iso3element(MONOLING_E_1),
                       encoding="unicode") == "<eng />"
    assert ET.tostring(monolinge_2_iso3element(MONOLING_E_6),
                       encoding="unicode") == \
           """<eng>Moscow<s n="np" /><s n="top" /></eng>"""


def append_leftiso3_rightiso3(bidixes):
    """ (List of String) -> (List of (String, String, String))

    Given a list with the names of bidix files, extract the language names
    and return a list with (bidix file name, lang1 iso3 code, lang2 iso3 code)
    tuples.
    ASSUME: bidix files are named following the standard:
            apertium-iso2or3-iso2or3.iso2or3-iso2or3.dix
    """
    res = []
    for bidix in bidixes:
        try:
            parse = re.search(r'.*apertium-([^-]+)-([^-]+).\1-\2.dix', bidix)
            lang1_iso3 = ISO2_2_ISO3.get(parse.group(1), parse.group(1))
            lang2_iso3 = ISO2_2_ISO3.get(parse.group(2), parse.group(2))
            res.append((bidix, lang1_iso3, lang2_iso3))
        except AttributeError:
            raise ValueError("Couldn't figure out the source language and "
                             "target language's iso codes from the bidix name!")
    return res

def test_append_leftiso3_rightiso3():
    assert append_leftiso3_rightiso3(['../apertium-kaz-tat.kaz-tat.dix',
                                      '/home/foo/apertium-tt-ky.tt-ky.dix',
                                      'apertium-ug-kaz.ug-kaz.dix']) == \
           [('../apertium-kaz-tat.kaz-tat.dix', 'kaz', 'tat'),
            ('/home/foo/apertium-tt-ky.tt-ky.dix', 'tat', 'kir'),
            ('apertium-ug-kaz.ug-kaz.dix', 'uig', 'kaz')]


def get_bidixes(apertium_root, skip_folders, iso_codes):
    """ String (List of String) (List of String) -> (List of String)

    Return the paths to all bidixes in apertium_root repo, in which both sl and
    tl are a language in iso_codes (except for bidixes in skip_folders)
    """

    def is_skippable(filepath):
        """ String -> Boolean

        Given a path to a bidix file, return True if it is located in
        a folder which should be skipped (code in branches/,release/ or similar).
        """
        for f in skip_folders:
            if f in filepath:
                return True
        return False

    res = []
    for filename in glob.iglob(apertium_root + '**/*.dix', recursive=True):
        if not is_skippable(filename):
            basename = os.path.basename(filename)
            for frst_iso in iso_codes:
                for scnd_iso in iso_codes:
                    if basename == "apertium-{0}-{1}.{0}-{1}.dix".format(frst_iso,
                                                                         scnd_iso):
                        res.append(filename)
    print('\n'.join(res), file=sys.stderr)
    return res


## Formatters
## ----------


def wordgraph2sexp(wg):
    """ WordGraph -> String

    Return s-expression representation of wg.
    """

    def me2sexp(me):
        return '(' + me.lang + ' "' + me.lm + '" (' + \
               ' '.join(me.tags) + '))'

    return '(' + '\n '.join(me2sexp(k) + \
                           ' (' + \
                           ' '.join(me2sexp(n) for n in sorted(list(v))) + \
                           ')' \
                           for k, v in wg.items()) + \
           ')'

def test_wordgraph2sexp():
    expected = \
    """
    ((eng "file" (n)) ((kaz "егеу" (n))
                       (kaz "файл" (n)))
     (kaz "файл" (n)) ((eng "file" (n)))
     (kaz "егеу" (n)) ((eng "file" (n))))
    """
    assert " ".join(wordgraph2sexp(WG_1).split()) == " ".join(expected.split())


## Runner
## ======

#    print(main(MAIN_BIDIX, RELEVANT_ISOS))

#+end_src

***** A script for converting English Wordnet leammas into Turkic languages

#+name: enwordnet2twordnet.py
#+begin_src python :exports yes :results output :tangle enwordnet2twordnet.py

## enwordnet2wordnet.py
##
## A script which walks over the synsets in the English Wordnet and prints
## translations for each English lemma in each synset using Google Translate
## (gt), Yandex Translate (yt) and looking them up in Apertium (ap) bilingual
## dictionaries (turned into a multilingual word graph beforehand).
##
## USAGE: python3 enwordnet2twordnet.py
##
## A snippet from the current output:
##
## def: (botany) a living organism lacking the power of locomotion
## ex: []
##     eng: plant
##         aze-gt: bitki?
##         aze-yt: zavod?
##         bak-yt: завод?
##         kaz-ap: кәсіпорын?
##         kaz-ap: өсімдік?
##         kaz-ap: фабрика?
##         kaz-ap: зауыт?
##         kaz-ap: қондырғы?
##         kaz-ap: көшет?
##         kaz-gt: өсімдік?
##         kaz-yt: зауыт?
##         kir-gt: өсүмдүк?
##         kir-yt: завод?
##         tat-ap: комбинат?
##         tat-ap: үсемлек?
##         tat-ap: завод?
##         tat-yt: завод?
##         tur-gt: bitki?
##         tur-yt: bitki?
##         uzb-gt: o&#39;simlik?
##         uzb-yt: o'simlik?
##     eng: flora
##         aze-gt: flora?
##         aze-yt: Flora?
##         bak-yt: Флора?
##         kaz-ap: флора?
##         kaz-gt: өсімдіктер?
##         kaz-yt: Флора?
##         kir-gt: өсүмдүктөр?
##         kir-yt: Флора?
##         tat-ap: флора?
##         tat-yt: Флора?
##         tur-gt: bitki örtüsü?
##         tur-yt: flora?
##         uzb-gt: flora?
##         uzb-yt: o'simlik?
##     eng: plant life
##         aze-gt: bitki həytı?
##         aze-yt: həyt bitkilər ?
##         bak-yt: үҫемлектәр тормошо ?
##         kaz-gt: Өсімдіктердің өмірі?
##         kaz-yt: өсімдіктердің өмірі ?
##         kir-gt: өсүмдүктөрдүн жашоо?
##         kir-yt: ак-өсүмдүктөрдүн ?
##         tat-yt: тормыш үсемлекләр ?
##         tur-gt: bitki haytı?
##         tur-yt: bitki yaşamı?
##         uzb-gt: o&#39;simlik hayoti?
##         uzb-yt: o'simlik hayoti?
## <...>
## Full output is in the xnet/ folder.

import nltk
nltk.data.path.append(r"/home/selimcan/local/nltk_data")
from nltk.corpus import wordnet as wn
from yandex_translate import YandexTranslate  ## pip install yandex.translate
from googleapiclient.discovery import build

import wordgraph as wg


############
## Constants


APERTIUM_ROOT = '../apertium-all/'

## from here: http://wiki.apertium.org/wiki/Turkic-languages
RELEVANT_ISOS =  ['kaz', 'kz', 'tat', 'tt', 'kir', 'ky', 'tyv', 'tur', 'tr',
                  'chv', 'cv', 'kum', 'kaa', 'uzb', 'uz', 'sah', 'crh', 'krc',
                  'bak', 'ba', 'nog', 'gag', 'tuk', 'tk', 'uig', 'ug', 'kjh',
                  'ota', 'aze', 'az', 'eng', 'en']

SKIP_FOLDERS = ['release', 'branches']  ## only relevant for the old svn repo

MAIN_BIDIX = APERTIUM_ROOT + \
             'apertium-trunk/apertium-eng-kaz/apertium-eng-kaz.eng-kaz.dix'

APERTIUMPOS_2_WNPOS = {'n': wn.NOUN, 'v': wn.VERB, 'adj': wn.ADJ, 'adv': wn.ADV}

POS = 'n'

GT_API_KEY = 'get one yourself if you need to'

GT = build('translate', 'v2', developerKey=GT_API_KEY)

YAT_API_KEY = 'get one yourself if you need to'

YAT = YandexTranslate(YAT_API_KEY)

AWG = wg.manytags2singletag(
          wg.bidixes2wordgraph(
              wg.append_leftiso3_rightiso3(
                  wg.get_bidixes(APERTIUM_ROOT, SKIP_FOLDERS, RELEVANT_ISOS))))

TURKIC = ['alt', 'aze', 'bak', 'chv', 'crh', 'gag', 'kaa', 'kaz', 'kir', 'kjh',
          'krc', 'kum', 'nog', 'ota', 'sah', 'tat', 'tuk', 'tur', 'tyv', 'uig',
          'uzb']

TURKIC_IN_GT = {'aze','kaz', 'kir', 'tur', 'uzb'}

TURKIC_IN_YAT = {'aze', 'bak', 'kaz', 'kir', 'tat', 'tur', 'uzb'}


############
## Functions


def yat_translate(s, lang1, lang2):
    """ (String String String) -> String

    Translate lang1 string s to lang2 with Yandex Translate.
    """
    return ' '.join(YAT.translate(s, lang1 + '-' + lang2)['text'])


def gt_translate(s, lang1, lang2):
    """ (String String String) -> String

    Translate lang1 string s to lang2 with Google Translate.
    """
    return GT.translations().list(source=lang1,
                                  target=lang2, q=s).execute()['translations'][0]['translatedText']


#########
## Runner


for s in list(wn.all_synsets(POS))[:10]:
    print('def:', s.definition())
    print('ex:', s.examples())
    for l in s.lemmas():
        l = l.name().replace('_', ' ')
        print('    eng:', l)
        for lang in TURKIC:
            seen = set()
            try:
                nbrs = AWG[wg.MonolingEntry('eng', l, (POS,))]
                for n in nbrs:
                    if n.lang == lang and n.lm not in seen:
                        print('        ' + lang + '-ap:', n.lm + '?')
                        seen.add(n.lm)
            except KeyError:
                try:
                    nbrs = AWG[wg.MonolingEntry('eng', l, ())]
                    for n in nbrs:
                        if n.lang == lang and n.lm not in seen:
                            print('        ' + lang + '-ap:', n.lm + '?')
                            seen.add(n.lm)
                except KeyError:
                    pass
            if lang in TURKIC_IN_GT:
                print('        ' + lang + '-gt:',
                      gt_translate(l, 'eng', lang) + '?')
            if lang in TURKIC_IN_YAT:
                print('        ' + lang + '-yat:',
                      yat_translate(l, 'en', wg.ISO3_2_ISO2[lang]) + '?')

#+end_src

Putting it into action:

#+name: twordnet
#+begin_src sh :exports results :results silent
# python3 enwordnet2twordnet.py > xnet/nouns.twn
#+end_src

*** Parallel corpus

Conditions:

- already available for the max. number of Turkic languages
- free license
- contemporary language

Options:

- Bibel

- Quran. Available in
  - kaz (from kuran.kz; in turkiccorpora;
    - TODO contact authors -- sharing on tanzil.net? (via Tanzil it will end up
      in OPUS)
    - TODO reformat to conform tanzil format if the answer is yes)
  - tat (in turkiccorpora; few other not OCR'd)
  - kir (TODO add to turkiccorpora; available [[http://www.quran-ebook.com/][here]] and [[https://archive.org/details/TranslationOfTheMeaningOfTheNobleQuranInTheKYRGYZKIRGHIZLanguageHQ][here]]) :GCI:
  - tyv?
  - tur * 10 (TODO add to turkiccorpora; available on tanzil.net)
  - chv (yes, but couldn't find online. Available upon request in electronic
    for, the author of it says in an interview)
  - kum?
  - kaa?
  - uzb (TODO add to turkiccorpora; available on tanzil.net)
  - sah?
  - crh (TODO add to turkiccorpora; available [[http://crimean.org/islam/koran/dizen-qurtnezir][here]]) :GCI:
  - krc (TODO convert to plain text; available in: turkiccorpora/dev) :GCI:
  - bak (TODO convert to plain text; available in: turkiccorpora/dev) :GCI:
  - nog?
  - gag?
  - tuk (yes, but couldn't find online)
  - uig (TODO add to turkiccorpora; available on tanzil.net)
  - kjh?
  - ota (probably not OCR'd)
  - aze * 2 (TODO add to turkiccorpora; available on tanzil.net) :GCI:

Also see:

- [[http://wiki.apertium.org/wiki/Parallel_corpus_pruning]]
  
** Connecting nodes

*** Nodes: Morphological Transducers

21 is the number of Turkic languages identified on
[[http://wiki.apertium.org/wiki/Turkic_languages]], but, according to a source
cited on the `Turkic languages' article on Wikipedia, there are at least 35 of
them. This means that in total about 35 morphological transducers will have to
be developed or generated (or just brought to a production-level coverage,
since many transducers already exist in the Apertium project, see `What was
done' section(s) above. Production-level coverage by the Apertium community is
defined as above 95% coverage on a range corpora. For the rest 5% of words or
so, we'd like the transducer/tagger to probabilistically guess the correct
tags, so that technically no out-of-vocabulary (OOV) words are left in the
output of a transducer.

*** To English

apertium-eng-kaz, apetium-tat-eng,

*** To Russian

apertium-kaz-rus, apertium-tat-rus,

*** Intraturkic

**** apertium-kaz-tat, apertium-tur-tat, apertium-crh-tur

** QA and meta-stuff

*** apertium fitnesse

See https://gitlab.com/selimcan/apertium-fitnesse and
http://fitnesse.selimcan.org.

*** rbmt-as-a-data-structure = a (Racket-based?) programming language with a syntax similar to what is seen on [[http://fitnesse.selimcan.org/FrontPage.ApertiumTurkic.ApertiumKaz]].

**** Rationale

Data-driven methods seem to win. The philosophy here is an old and simple one:
to generate data using a rule-based system, fix errors, and use that as a
feedback for improving the rule-based system (or train a statistical/hybrid
system). Somewhat new idea is to make this improving happen on the fly, in a
loop, so that we can generate descent training data even faster. The goal is to
shorten the time it takes to improve a translator in the light of the feedback
given. That is, ideally it has to be a fully automatic process.

**** Code

#+name: annotate.py
#+begin_src python :exports yes :results output :tangle annotate.py
#!/usr/bin/env python3

"""
annotate.py: a script for semi-automacally annotating texts *by using* and *for
             improving* an Apertium machine translator (or training other
             machine translators).

INPUT: 6 column, one-token-per-line text in the following format:

|surface form |lemma |tags |lexicon |lexicalAffixes |correctlySpelled|

In the input, any column, except for the first one with surface forms, can be
empty:

|урманнар|||||

What annotate.py will do is it will fill in the rest of the columns:

|урманнар|урман|n pl nom|N1||

If a cell is already filled in in the input, annotate.py will leave it as it is.

Such already-filled-in cells serve as training data for annotate.py for guessing
the lemma & lexicon & possibly correct spelling (in cases where the surface form
is unknown for the Apertium's morphological transducer), or for selecting
correct reading (in cases where Apertium returns several analyses for the given
surface form). The script will read in all of the input, train itself on the
already annotated part, and fill in the empty cells with its guesses.

Ultimately it will modiy the Apertium transducer in place, or spit out a new
version of it, after having seen the annotated data.

The `lexical affixes' cell might stay empty even in the output of annotate.py,
but the cell itself has to be there.

An example of a token where the `lexical affixes' field is not empty:
|урманнар|урман|n pl nom|N1|урман>LAр|

For tokens which were misspelled (or incorrectly OCR'd) in the original, there
can be an optional sixth cell, where the correct spelling of the surface form
is given.

The reason for putting the correct spelling in an additional cell and keep
the original spelling as it is, is that the data about misspellings is a
valuable thing to have (for training an automatic spelling corrector, in
particular).

Once a particular piece of text is fully annotated, we encourage you to add it
to our shared corpus in the `corpus' directory in the repo of the Apertium
morphological analyser in question (if the license of the text allows that),
with meta-information about the courpus in the following format, and submit
a pull request:
 
BEGIN EXAMPLE
<corpus>
  <doc title="Кішкентай ханзада" author="А. де Сент-Экзюпери"
       translator="Ж. Қонаева" pub="2013" lang="kk" origlang="fr"
       source="kitap.kz/12345/abcde.html" license="allRightsReserved"
       annotators="Мәхмүт Салықтөлеуші (optional@email.com)">
    <p>
      <s>Бірде, алть жастағы кезімде [...]
        <t>|Бірде|бірде|adv|ADV||</t>
        <t>|алть|алты|num|NUM||алты|</t>
        [...]
      </s>
      [...]
   </p>
  </doc>    
</corpus>
END EXAMPLE

USAGE:
"""

from collections import namedtuple


##################
## Data definitons


## Token is a Token(String, String, ListOfString, String, String, StringOrNone).

T_0 = ["урманнар", "", [], "", "", None]
T_1 = ["урманнар", "урман", ["n", "pl", "nom"], "N1", "", None]
T_2 = ["алть", "алты", ["num", "pl", "nom"], "N1", "", "алты"]

#+end_src

*** Problem 404

Rationale: out-of-vocabulary words lead to not firing transfer rules. Not firing
transfer rules lead to bad translation. Bad translation leads to sadness.

Instance of: stemming, lemmatization, sequence labeling, pos tagging,
             classification, inference in graphical models (depending on how
             exactly it is formulated)

Possible formulations:

**** Problem 404.a

#+begin_src text
## INPUT-1:
## ...
## ^анасы/ана<n><px3sp><nom>/ана<n><px3sp><nom>+и<cop><aor><p3><pl>/ана<n><px3sp><nom>+и<cop><aor><p3><sg>$
## ^хәйрелниса/*хәйрелниса$
## ^Нәҗметдин/Нәҗметдин<np><ant><m><nom>/Нәҗметдин<np><ant><m><nom>+и<cop><aor><p3><pl>/Нәҗметдин<np><ant><m><nom>+и<cop><aor><p3><sg>$
## ^кызы/кыз<n><px3sp><nom>/кыз<n><px3sp><nom>+и<cop><aor><p3><pl>/кыз<n><px3sp><nom>+и<cop><aor><p3><sg>$
## ...
##
## OUTPUT-1:
## ...
## ^анасы/ана<n><px3sp><nom>/ана<n><px3sp><nom>+и<cop><aor><p3><pl>/ана<n><px3sp><nom>+и<cop><aor><p3><sg>$
## ^хәйрелниса/хәйрелниса<np><ant><f><nom>$
## ^Нәҗметдин/Нәҗметдин<np><ant><m><nom>/Нәҗметдин<np><ant><m><nom>+и<cop><aor><p3><pl>/Нәҗметдин<np><ant><m><nom>+и<cop><aor><p3><sg>$
## ^кызы/кыз<n><px3sp><nom>/кыз<n><px3sp><nom>+и<cop><aor><p3><pl>/кыз<n><px3sp><nom>+и<cop><aor><p3><sg>$
## ...
#+end_src

That is, classes are entire tag sequences, no stemming or lemmatization
required. Issue: probably too many classes to be feasible without a gazillion
gigabytes of training data (although there are papers on multiclass
classification for cases when there are even more, google 'Training Highly
Multiclass Classifiers' for an example)

For example, if we run Tatar Quran through apertium-tat:

#+name: uniq-tag-sequences
#+begin_src sh :exports both
cat ../../../turkiccorpora/tat.quran.nughmani.txt | \
apertium -d ../../apertium-languages/apertium-tat tat-tagger | \
grep -oP "(<[[:alnum:]]+>)*" | sort | uniq | wc -l
#+end_src

we get

#+RESULTS: uniq-tag-sequences
: 991

uniq tag sequences.

#+begin_src text
n-px3sp-nom  ->  ?  ->  np-ant-m-nom  -> n-px3sp-nom
    |            |           |                |
  анасы      хәйрелниса  Нәҗметдин          кызы

P(n-px3sp-nom), P(np-ant-m-nom), P(n-px3sp-nom) = 1
#+end_src

What we want instead of the ? is (not too sure)

- probability distribution of 991 tag sequences observed ?
- a tag that maximizes the probability of the entire sequence ?

Belief propagation?

**** Problem 404.b

#+begin_src text
## INPUT-2:
## ...
## ^анасы/ана N1$
## ^хәйрелниса/*хәйрелниса$
## ^Нәҗметдин/Нәҗметдин NP-ANT-M$
## ^кызы/кыз N1$
## ...
##
## OUTPUT-2:
## ...
## ^анасы/ана N1$
## ^хәйрелниса/Хәйрелниса NP-ANT-F$
## ^Нәҗметдин/Нәҗметдин NP-ANT-M$
## ^кызы/кыз N1$
## ...
#+end_src

This doesn't solve the original problem, and rather might help with expanding
dictionaries.

**** Problem 404.c

#+begin_src text
## INPUT-3:
## ...
## ^анасы/ана<n><px3sp><nom>$
## ^хәйрелниса/*хәйрелниса$
## ^Нәҗметдин/Нәҗметдин<np><ant><m><nom>$
## ^кызы/кыз<n><px3sp><nom>$
## ...
##
## OUTPUT-3:
## ...
## ^анасы/ана<n><px3sp><nom>$
## ^хәйрелниса/Хәйрелниса<np><ant><f><nom>$
## ^Нәҗметдин/Нәҗметдин<np><ant><m><nom>$
## ^кызы/кыз<n><px3sp><nom>$
## ...
#+end_src

Same as in (1), but with lemmatization.

Background reading:

  - Apertium Tagger related
  - Zhenis et al.'s paper on Hybrid Kazakh disambiguation tool
  - NLTK on stemming, lemmatization and pos-tagging
  - Guessing with CG?
  - Y&M on stemming, lemmatization and pos-tagging
  - M&S on the same

* ROADMAP

* NOTES

** Methods of auditing a monolingual dictionary

- Take stems contained in it and pass them through the transducer to see whether
  they get multiple analyses (some of which might be wrong). Better yet, use the
  ~lexc2dix~ library, parse the .lexc file with it, and get the list of stems
  which are linked to 2 or more continuation lexicons.

** Principles of tagset choice

- surface form has to be deterministically reconstructuble from lemma + tags /
  subreadings

** Minimal monolingual Apertiumpp package

- transducers + disambiguator
- spellchecker
- annotated corpus with an interface
- virtual keyboard
- Common Voice's interface translations
- sentences for Common Voice
